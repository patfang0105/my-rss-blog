name: Cache RSS Feeds

on:
  schedule:
    - cron: '0 */6 * * *'  # 每6小时运行一次
  workflow_dispatch: # 允许手动触发

permissions:
  contents: write

jobs:
  cache-feeds:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests
    
    - name: Fetch and cache RSS feeds
      run: |
        python3 << 'EOF'
        import requests
        import json
        import os
        from datetime import datetime

        FEEDS = [
            "https://patfang0105.github.io/my-rss-feeds/rss_www_csis_org.xml",
            "https://patfang0105.github.io/my-rss-feeds/rss_www_cfr_org.xml",
            "https://www.atlanticcouncil.org/feed/",
            "https://www.imf.org/en/publications/rss?language=eng",
            "http://project-syndicate.org/rss",
            "https://rhg.com/feed/",
            "https://www.aei.org/feed/",
            "https://www.wto.org/library/rss/latest_news_e.xml",
            "https://www.foreignaffairs.com/rss.xml",
            "https://www.piie.com/rss/update.xml",
            "https://amro-asia.org/feed/",
            "https://politepol.com/fd/X589A2bsRjn8.xml"
        ]

        # 创建缓存目录
        os.makedirs('cached_feeds', exist_ok=True)

        results = []
        for url in FEEDS:
            try:
                print(f"Fetching {url}...")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                
                # 保存原始 XML
                filename = url.replace('https://', '').replace('http://', '').replace('/', '_').replace('?', '_').replace('=', '_')
                filepath = f'cached_feeds/{filename}'
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                results.append({
                    'url': url,
                    'status': 'success',
                    'cached_file': filename,
                    'size': len(response.text)
                })
                print(f"✓ Cached {url} ({len(response.text)} bytes)")
                
            except Exception as e:
                print(f"✗ Failed to fetch {url}: {e}")
                results.append({
                    'url': url,
                    'status': 'failed',
                    'error': str(e)
                })

        # 保存元数据
        metadata = {
            'last_update': datetime.utcnow().isoformat() + 'Z',
            'feeds': results
        }
        
        with open('cached_feeds/metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\n✓ Cached {len([r for r in results if r['status'] == 'success'])}/{len(FEEDS)} feeds")
        EOF
    
    - name: Check for changes
      id: verify-changed-files
      run: |
        if [ -n "$(git status --porcelain)" ]; then
          echo "changed=true" >> $GITHUB_OUTPUT
        else
          echo "changed=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Commit and push changes
      if: steps.verify-changed-files.outputs.changed == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add cached_feeds/
        git commit -m "Auto-cache RSS feeds - $(date -u +%Y-%m-%d_%H:%M:%S)"
        git push

